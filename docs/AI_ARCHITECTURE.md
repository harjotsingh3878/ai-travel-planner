# Production-Grade AI Architecture: Travel Planner

This document describes the architecture, implementation patterns, and operational practices for the AI-powered travel planning system. It is suitable for senior engineering review and interview discussion.

---

## 1. ARCHITECTURE

### 1.1 High-Level Diagram

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                              CLIENT (Next.js App)                                 │
│  Create Trip Form → Server Action (createTrip) → Loading / Result UI              │
└─────────────────────────────────────────────────────────────────────────────────┘
                                          │
                                          ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│                         AI ORCHESTRATION LAYER (Server)                           │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────────────┐  │
│  │ Rate Limit   │  │ Token Quota   │  │ RAG Retrieval│  │ Prompt Builder       │  │
│  │ Check        │  │ Check         │  │ (pgvector)   │  │ (system + context)   │  │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘  └──────────┬───────────┘  │
│         │                 │                 │                       │             │
│         └─────────────────┴─────────────────┴───────────────────────┘             │
│                                          │                                         │
│                          ┌──────────────▼──────────────┐                          │
│                          │  LLM Call (OpenAI / Gemini)  │                          │
│                          │  + Optional Tool Calls       │                          │
│                          └──────────────┬──────────────┘                          │
│                                          │                                         │
│  ┌──────────────────────────────────────▼──────────────────────────────────────┐  │
│  │  Structured Output: Zod parse → validate → retry on failure → persist       │  │
│  └──────────────────────────────────────┬──────────────────────────────────────┘  │
│                                          │                                         │
│  ┌──────────────────────────────────────▼──────────────────────────────────────┐  │
│  │  Token Usage Log → Supabase (usage table)                                    │  │
│  └──────────────────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────────┘
                                          │
          ┌──────────────────────────────┼──────────────────────────────┐
          ▼                              ▼                              ▼
┌─────────────────┐           ┌─────────────────┐           ┌─────────────────┐
│  Supabase       │           │  Supabase        │           │  Supabase        │
│  trips (JSONB)  │           │  embeddings      │           │  ai_usage        │
│  users          │           │  (pgvector)      │           │  (token logs)    │
└─────────────────┘           └─────────────────┘           └─────────────────┘
```

### 1.2 Component Roles

| Component | Responsibility |
|-----------|----------------|
| **RAG** | Embed user query + destination; retrieve cities, attractions, visa rules, past itineraries from vector store; inject as context into the prompt. |
| **Embeddings** | Stored in Supabase via `pgvector`; generated by same provider’s embedding API (OpenAI or Gemini) for consistency. |
| **Prompt orchestration** | Build system prompt (role, constraints, anti-hallucination rules), inject RAG context, add user input and optional tool results. |
| **Structured outputs** | Enforce strict JSON schema; validate with Zod; retry with simplified prompt or fallback provider on validation failure. |
| **Tools** | Weather, distance, budget calculator, currency conversion; LLM requests tools when needed; orchestrator executes and appends results to the conversation. |

### 1.3 Data Flow (Step-by-Step)

1. **User** submits trip form (destination, days, budget, style, interests).
2. **Server Action** `createTrip` runs: auth check → rate limit → token quota check.
3. **RAG**: Embed `destination + interests + travel_style`; query `travel_embeddings` for top-k similar items (cities, attractions, visa, past itineraries); format as context string.
4. **Prompt build**: System prompt + RAG context + user trip details; if tools are used, append tool definitions and (in a loop) tool calls + results.
5. **LLM call**: Primary provider (e.g. Gemini); on failure or quota, fallback to OpenAI. Optionally use structured output (e.g. OpenAI `response_format: { type: 'json_object' }`) or post-parse with Zod.
6. **Parse & validate**: JSON parse → Zod schema; on failure, retry (e.g. once with “fix this JSON” prompt, then fallback provider).
7. **Persist**: Save trip to `trips`; log token usage to `ai_usage`.
8. **Response**: Return structured itinerary to client; UI renders day-by-day plan.

---

## 2. RAG IMPLEMENTATION

### 2.1 Why pgvector

- **Single store**: Supabase/PostgreSQL already used; no extra vector DB.
- **ACID + vectors**: Same DB for trips, users, and embeddings; simpler ops and backups.
- **Supabase support**: pgvector is supported; use `vector` type and `<=>` or `<->` for similarity.

### 2.2 Schema (Supabase/PostgreSQL)

```sql
-- Enable pgvector
CREATE EXTENSION IF NOT EXISTS vector;

-- Embeddings table: one row per chunk (city, attraction, visa rule, or itinerary summary)
CREATE TABLE travel_embeddings (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  content_type VARCHAR(50) NOT NULL,  -- 'city' | 'attraction' | 'visa_rule' | 'itinerary_summary'
  source_id UUID,                     -- optional FK to trips.id for itinerary_summary
  content TEXT NOT NULL,
  metadata JSONB DEFAULT '{}',        -- e.g. { "city": "Paris", "country": "France" }
  embedding vector(1536) NOT NULL,    -- OpenAI ada-002 dimension; use 768 for Gemini
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_travel_embeddings_type ON travel_embeddings(content_type);
CREATE INDEX idx_travel_embeddings_embedding ON travel_embeddings
  USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

- **Index**: `ivfflat` is good for up to ~1M rows; for larger scale use `hnsw` (faster search, slightly more build cost).
- **Dimension**: Must match embedding model (e.g. 1536 for `text-embedding-3-small`, 768 for Gemini embedding).

### 2.3 What to Index

| content_type | Source | Example content |
|--------------|--------|------------------|
| city | Curated or scraped docs | "Paris: major sights, best seasons, transport, safety." |
| attraction | POI database or docs | "Eiffel Tower: hours, tickets, tips." |
| visa_rule | Official or trusted source | "France Schengen visa: 90/180 days, required docs." |
| itinerary_summary | User’s past trips (anonymized or per-user) | "3-day Paris: Louvre, Eiffel, Montmartre; budget ~$400." |

- **Past itineraries**: When a trip is saved, generate a short summary (or use the first N characters of the itinerary), embed it, and insert into `travel_embeddings` with `content_type = 'itinerary_summary'` and `source_id = trip.id`. RAG can filter by `user_id` (via metadata or a join) for personalization.

### 2.4 Retrieval and Injection

- **Query**: Embed the concatenated string: `destination + travel_style + interests.join(', ')`.
- **Search**: `SELECT content, metadata FROM travel_embeddings WHERE content_type = ANY($1) ORDER BY embedding <=> $2 LIMIT $3` (with bound types and limit, e.g. 10–20).
- **Safety**: 
  - Truncate each retrieved chunk to a max length (e.g. 500 chars) to avoid token overflow.
  - Put RAG context in a clearly labeled block: “Use the following verified information. Do not invent details that are not stated here.”
  - Prefer content from `content_type` in order: visa_rule > city > attraction > itinerary_summary (so factual rules override generic suggestions).

Pseudocode:

```ts
const context = chunks
  .map(c => `[${c.content_type}] ${c.content}`)
  .join('\n\n');
const userMessage = `${RAG_BLOCK}\n${context}\n\n${USER_TRIP_PROMPT}`;
```

---

## 3. PROMPT ENGINEERING

### 3.1 System Prompt (Robust Itinerary Generation)

- **Role**: “You are an expert travel planner. You only use verified information from the context below or from tool results. Do not invent addresses, opening hours, or prices not provided.”
- **Output format**: “Respond with exactly one JSON object matching the provided schema. No markdown, no code fences, no extra text.”
- **Constraints**: “Budget is in USD; total estimated cost must not exceed the user’s budget. Respect the number of days and travel style (budget/moderate/luxury).”
- **Anti-hallucination**: “If context does not contain enough detail for a specific activity, describe it in general terms and do not fabricate names, times, or prices.”

### 3.2 Enforcing Budget, Dates, and Travel Style

- In the **user** block, state explicitly: “Budget: $X USD total. Duration: N days. Travel style: Y. Do not suggest a total cost above $X.”
- In **post-processing** (or in a tool): Sum activity costs and daily totals; if over budget, either re-prompt (“Total was $Z; reduce to under $X”) or cap/scale in code and note “adjusted to fit budget” in the UI.

### 3.3 Reducing Hallucinations

- **RAG first**: Prefer retrieved facts over model memory.
- **Cite context**: In the schema, optionally add a `source` or `context_used` field so the model can reference “from context” for key facts.
- **Lower temperature**: e.g. 0.3–0.5 for itinerary generation.
- **Structured output**: Strict schema reduces free-form fabrication.
- **Tool use**: Use weather/currency/distance tools instead of the model guessing numbers.

---

## 4. STRUCTURED OUTPUTS

### 4.1 Strict JSON Schema (Align with Zod)

```ts
// Zod schema - single source of truth for validation
const ActivitySchema = z.object({
  time: z.string().regex(/^\d{1,2}:\d{2}\s*(AM|PM)$/i),
  name: z.string().min(1).max(200),
  description: z.string().min(1).max(1000),
  location: z.string().min(1).max(300),
  cost: z.number().min(0),
  duration: z.string().min(1).max(100),
});

const DayItinerarySchema = z.object({
  day: z.number().int().min(1),
  title: z.string().min(1).max(200),
  activities: z.array(ActivitySchema).min(1).max(20),
  estimated_cost: z.number().min(0),
  tips: z.array(z.string().max(500)).max(10),
});

const ItineraryOutputSchema = z.object({
  itinerary: z.array(DayItinerarySchema),
  total_estimated_cost: z.number().min(0),
});
```

- Expose this same shape in the **prompt** as “Return only a JSON object with this exact structure.”

### 4.2 Validation and Retry

1. Parse `JSON.parse(response)`; on throw, treat as invalid.
2. Run `ItineraryOutputSchema.safeParse(parsed)`.
3. If valid, return typed result.
4. If invalid:
   - **Retry 1**: Append “Your previous response had validation errors: {errors}. Return valid JSON only.” and call LLM again (same provider).
   - **Retry 2**: Switch to fallback provider (e.g. Gemini → OpenAI) and same user message.
   - After max retries, return a user-friendly error and optionally log the raw response for debugging.

### 4.3 Fallback Strategy Summary

| Step | Action |
|------|--------|
| 1 | Parse JSON; if fail → retry with “Return valid JSON only.” |
| 2 | Zod validate; if fail → retry with error details. |
| 3 | If still fail → one attempt with fallback provider. |
| 4 | If all fail → return error; log prompt + response for ops. |

---

## 5. TOOL / FUNCTION CALLING

### 5.1 Proposed Tools

| Tool | Purpose | When the LLM might call it |
|------|--------|-----------------------------|
| **get_weather** | Forecast for city + date | When suggesting outdoor activities or packing. |
| **get_distance** | Distance/duration between two places | When ordering activities or estimating transport time. |
| **budget_calculator** | Allocate budget across days/categories | When enforcing budget or splitting costs. |
| **currency_convert** | Convert amount from one currency to another | When user budget is in non-USD or comparing costs. |

### 5.2 Tool Definitions (OpenAI-style)

```ts
const tools = [
  {
    type: 'function',
    function: {
      name: 'get_weather',
      description: 'Get weather forecast for a city on a given date. Use when suggesting outdoor activities.',
      parameters: {
        type: 'object',
        properties: {
          city: { type: 'string', description: 'City name' },
          date: { type: 'string', description: 'ISO date YYYY-MM-DD' },
        },
        required: ['city', 'date'],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'currency_convert',
      description: 'Convert amount between currencies (e.g. USD to EUR).',
      parameters: {
        type: 'object',
        properties: {
          amount: { type: 'number' },
          from_currency: { type: 'string' },
          to_currency: { type: 'string' },
        },
        required: ['amount', 'from_currency', 'to_currency'],
      },
    },
  },
  // budget_calculator, get_distance similar
];
```

### 5.3 Usage Flow

1. **First LLM call**: Messages = [system, user with RAG context]; `tools` = tool definitions; `tool_choice` = `"auto"`.
2. **If** `response.choices[0].message.tool_calls` is present:
   - For each `tool_call`: resolve by name to a handler (e.g. `get_weather` → call weather API or mock).
   - Append to messages: assistant message with `tool_calls`; then for each call, a tool message with `tool_call_id` and result.
   - Call LLM again with updated messages; repeat until no more tool calls or max iterations (e.g. 3).
3. **Final** assistant message (no tool calls) is the itinerary text; then run JSON parse + Zod as above.

- **Gemini**: Use Gemini’s function-calling API (e.g. `generateContent` with tools and `functionCall`/`functionResponse` in the same loop).

---

## 6. COST, SAFETY & RELIABILITY

### 6.1 Token Usage Tracking (Per User)

- **Table**: `ai_usage` with `user_id`, `model`, `input_tokens`, `output_tokens`, `total_cost` (optional), `request_id`, `created_at`.
- **Computation**: From provider response (e.g. `usage.prompt_tokens`, `usage.completion_tokens`); cost = input * price_in + output * price_out (per-model table or env config).
- **Where to log**: In the same function that calls the LLM, after each successful completion (and optionally on fallback attempts). Use a single “logUsage” helper.

### 6.2 Rate Limiting and Quotas

- **Per user**: e.g. max N itinerary requests per hour (in-memory or Redis: key = `ratelimit:ai:{user_id}`, increment, TTL 1h).
- **Per user quota**: Max tokens per day/month from `ai_usage`; before calling LLM, `SELECT SUM(input_tokens + output_tokens) FROM ai_usage WHERE user_id = $1 AND created_at > $2`; if over threshold, return 429 with “Daily token quota exceeded.”
- **Global**: Optional global cap or queue to avoid provider rate limits.

### 6.3 Fallback Between Gemini and OpenAI

- **Order**: Try primary provider (e.g. Gemini) first. On failure (network, 5xx, quota, or parse/validation failure after retries), call the same flow with OpenAI.
- **Idempotency**: Same request id so logs show “tried Gemini, then OpenAI.” Do not double-count tokens; only log the provider that succeeded (or both if you bill for attempts).

### 6.4 Logging and Monitoring

- **Log**: request_id, user_id, provider, model, token counts, latency, validation success/failure, and whether fallback was used. Do not log full prompts or responses in production (or redact PII).
- **Metrics**: Emit (e.g. to Datadog/CloudWatch): `ai.requests`, `ai.tokens.input`, `ai.tokens.output`, `ai.latency`, `ai.validation_failures`, `ai.fallback_used`. Alert on high validation failure rate or error rate.

---

## 7. SCALABILITY & INTERVIEW READINESS

### 7.1 Scaling to 100k+ Users

- **Stateless API**: Next.js Server Actions / API routes are stateless; scale horizontally behind a load balancer.
- **DB**: Supabase/PostgreSQL: connection pooling (e.g. PgBouncer), read replicas for RAG and analytics; keep writes on primary.
- **Embeddings**: Batch re-indexing in a job queue (e.g. Inngest, Bull); avoid re-embedding on every request; cache frequent RAG queries per (destination, style) if needed.
- **Rate limits & quotas**: Use Redis for distributed rate limiting and quota checks.
- **LLM**: Respect provider rate limits; use queues (e.g. background job for “generate itinerary”) so bursts are smoothed; consider multiple API keys or provider accounts for higher throughput.
- **Caching**: Cache final itinerary for identical input (hash of normalized trip params) with short TTL to reduce duplicate LLM calls.

### 7.2 Senior-Level Interview Q&A

**Q1: How do you balance RAG context size with prompt token limits and relevance?**  
**A:** We limit the number of retrieved chunks (e.g. top 10–20 by similarity), cap each chunk length (e.g. 500 chars), and put the most trusted content types first (visa > city > attraction). We also reserve token budget for the user message and the JSON output. If the model supports it, we use a separate “context” window or summarize the context in a second step to stay under limits.

**Q2: How would you detect and mitigate prompt injection in this system?**  
**A:** We treat RAG content as trusted but separate from the user message (e.g. “Verified context: …” vs “User request: …”). We don’t let raw user input override system instructions. For high-risk surfaces, we could run a classifier on the user message to detect injection attempts and reject or sanitize. We also avoid putting user-controlled text in the system role.

**Q3: Why Zod over other validation approaches for LLM output?**  
**A:** Zod gives a single schema that can generate TypeScript types and validate at runtime. We use it as the single source of truth for the expected JSON shape and for clear error messages on failure, which we can feed back into a retry prompt. Alternatives like JSON Schema are good for documenting the API for the LLM, but we still want runtime validation and type safety in the app.

**Q4: How do you handle LLM non-determinism in production?**  
**A:** We use a low temperature (0.3–0.5) and structured output to reduce variance. We validate with Zod and retry on failure. We log request_id and optionally seed where the provider allows. For critical paths, we might run two calls and compare or use a single preferred provider with fixed settings so behavior is more reproducible.

**Q5: How would you add A/B testing for different prompts or models?**  
**A:** We’d assign users or requests to variants (e.g. by hash of user_id or request_id). The orchestrator would select prompt version and/or model based on variant. We’d log variant, model, and success metrics (validation pass, user acceptance, follow-up actions) to the same `ai_usage` or analytics table, then analyze by variant to compare conversion or quality.

---

## Implementation Map

| Area | Location in codebase |
|------|----------------------|
| Zod schemas | `src/lib/ai/schema.ts` |
| RAG retrieval | `src/lib/ai/rag.ts` |
| Prompts | `src/lib/ai/prompts.ts` |
| Tools | `src/lib/ai/tools.ts` |
| Orchestrator (LLM + tools + validate + retry) | `src/lib/ai/orchestrator.ts` |
| Token tracking | `src/lib/ai/tokenTracking.ts` |
| Rate limit / quota | `src/lib/ai/rateLimit.ts` (or middleware) |
| DB migrations | `supabase/migrations/` for pgvector and `ai_usage` |

---

## Quick Reference: 5 Senior Interview Q&A

1. **Balance RAG context size vs token limits?** Limit top-k chunks, cap per-chunk length, order by trust (visa > city > attraction). Reserve budget for user message and JSON output.
2. **Prompt injection mitigation?** Separate "verified context" from "user request"; don’t put user-controlled text in system role; optional classifier for high-risk surfaces.
3. **Zod over other validation?** Single schema = types + runtime validation; clear errors for retry prompts; type-safe app code.
4. **LLM non-determinism in production?** Low temperature, structured output, Zod retry, optional seed; log request_id for reproducibility.
5. **A/B testing prompts/models?** Hash user_id or request_id to variant; log variant + success metrics in ai_usage/analytics; analyze by variant.

---

**Note:** If `OPENAI_API_KEY` is not set, RAG is skipped (orchestrator catches and continues without context). Embeddings use OpenAI for pgvector dimension 1536. For Gemini-only deployments, add a Gemini embedding path and a separate embeddings table with 768 dimensions, or keep RAG disabled.
